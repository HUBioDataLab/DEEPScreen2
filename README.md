# DEEPScreen2: Deep Learning for Drug-Target Interaction Prediction

Computational models for drug–target interaction (DTI) prediction aim to reduce the cost of experimental screening, but often require complex molecular representations and long development cycles. Here we present DEEPScreen++, a modular, open-source framework that performs DTI prediction directly from RDKit-generated 2D molecular images, expanded with augmentation, using convolutional neural network and vision transformer backbones. DEEPScreen++ streamlines data curation, image generation, training and inference into an end-to-end pipeline that can be completed within seven days for a new target on standard hardware. Prospective applications to de novo design and drug repurposing identified two experimentally confirmed active molecules and two repurposed drugs, respectively. DEEPScreen++ thus demonstrates that simple image-based models can match or exceed traditional descriptor-based approaches for high-accuracy DTI prediction and rapid hypothesis generation.

## Key Features

*   **Automated Data Handling**: Automatically downloads, processes, and splits data from ChEMBL, MoleculeNet, and TDC.
*   **State-of-the-Art Architectures**: Supports both **CNN** (optimized resnet-like) and **Vision Transformers (ViT)** (SwinV2-based).
*   **Robust Preprocessing**: 300x300 image generation with 36-fold rotational augmentation for rotation-invariant predictions.
*   **Hyperparameter Optimization**: Integrated WandB sweeps for automated hyperparameter tuning.
*   **Task Agnostic**: Applicable to any binary classification task (bioavailability, toxicity, binding affinity) across various benchmarks.
*   **Easy Deployment**: Simple CLI for training, evaluation, and inference.

## Quickstart (5-10 Minutes)

Get started with a simple run on a ChEMBL target using the default CNN model.

1.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

2.  **Run a Training Job**:
    This will download data for target `CHEMBL286`, split it, and train a model.
    ```bash
    python main_training.py \
        --target_id "CHEMBL286" \
        --assay_type "B" \
        --pchembl_threshold 6.0 \
        --model CNNModel1 \
        --en quickstart_demo \
        --project_name deepscreen_demo
    ```
    *Results will be saved in `result_files/experiments/` and `trained_models/`.*

## Installation

```bash
git clone https://github.com/HUBioDataLab/DEEPScreen2.git
cd DEEPScreen2
pip install -r requirements.txt
```

**Requirements**:
*   Python 3.x
*   PyTorch
*   RDKit
*   WandB (for logging and sweeps)
*   And others listed in `requirements.txt`

## Usage

DEEPScreen2 is controlled via `main_training.py` for training and `predict_deepscreen.py` for inference.

### 1. Task & Dataset Selection

**ChEMBL Target**:
```bash
python main_training.py --target_id CHEMBL286 --dataset chembl
```

**MoleculeNet Benchmark**:
```bash
python main_training.py --target_id bace --dataset moleculenet --scaffold --model ViT --muon
```

**Therapeutics Data Commons (TDC)**:
```bash
python main_training.py --target_id Bioavailability_Ma --dataset tdc
```

### 2. Architecture Selection

You can select the model architecture globally using the `--model` flag.

*   **CNN (Default)**: Optimized Convolutional Network.
    ```bash
    --model CNNModel1
    ```
*   **Vision Transformer (ViT)**: SwinV2-based Transformer.
    ```bash
    --model ViT --muon
    ```
    *Note: Use `--muon` to enable the Muon optimizer, which is recommended for ViT training.*

### 3. Hyperparameter Optimization

DEEPScreen2 supports automated hyperparameter tuning using Weights & Biases (WandB) Sweeps.

1.  **Configure Sweep**: Edit `sweep_config.yaml` to define your search space (learning rate, batch size, architectural parameters).
2.  **Run Sweep**:
    ```bash
    python main_training.py --sweep --project_name my_sweep_project --target_id CHEMBL286
    ```
    This will initialize a WandB sweep agent and start running experiments based on the configuration.

### 4. Prediction / Inference

Once a model is trained, use `predict_deepscreen.py` to screen new molecules.

```bash
python predict_deepscreen.py \
    --model_path trained_models/your_experiment_name/best_model.pth \
    --smiles_file prediction_files/your_compounds.csv \
    --target_id my_prediction_run
```
*   `--smiles_file`: A CSV containing SMILES strings to predict.
*   `--model_path`: Path to the `.pth` checkpoint file.

## Examples

Explore detailed use-cases in the `examples/` directory:

*   **[Monkeypox Viral DNA Polymerase](examples/monkeypox/README.md)**:
    Predicting active compounds against Monkeypox virus using data from DrugBank and literature.
*   **[DrugGEN Generated Molecules](examples/druggen/README.md)**:
    Screening molecules generated by the DrugGEN model for specific target activity.

## Project Structure

```
DEEPScreen2/
├── examples/               # Specific use-case examples (Monkeypox, DrugGEN)
├── main_training.py        # Main entry point for training
├── train_deepscreen.py     # Training logic and loops
├── predict_deepscreen.py   # Inference script
├── models.py               # Model definitions (CNNModel1, ViT)
├── data_processing.py      # Data loading and image generation
├── sweep_config.yaml       # Hyperparameter sweep configuration
├── config.yaml             # Default configuration
└── requirements.txt        # Python dependencies
```

## Coming Soon / In Progress

*   **YOLO Support**: We are working on integrating YOLO-based object detection architectures for more interpretable feature localization on molecular images. Stay tuned!

## Citations

If you use DEEPScreen2 in your research, please cite our relevant works:

```bibtex
@misc{unlu2025deepscreen,
  title={DEEPScreen++: A Modular Image-Based Deep Learning Framework for Drug–Target Interaction Prediction},
  author={{\"U}nl{\"u}, Atabey and {\c{C}}al{\i}{\c{s}}kan, Mehmet Furkan and {\.I}nan, Furkan Necati and {\"O}rer, Kemal and {\"O}rer, Kerem and Do{\u{g}}an, Tunca},
  year={2025},
  howpublished={\url{https://github.com/HUBioDataLab/DEEPScreen2}}
}
```

---
**License**: [MIT](LICENSE) | **Contributing**: Pull requests are welcome!